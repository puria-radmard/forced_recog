_wandb:
    value:
        cli_version: 0.21.3
        e:
            ldrbasgq9dmimqcbc6nl655staajz37n:
                args:
                    - configs/llama_3_forward_training_test.yaml
                    - temp0.0_stylesun
                cpu_count: 16
                cpu_count_logical: 32
                cudaVersion: "12.5"
                disk:
                    /:
                        total: "263624122368"
                        used: "50250153984"
                executable: /scratch/anaconda3/envs/myenv/bin/python
                git:
                    commit: 14dccb65481093c647438e409c4682d60cf228b8
                gpu: NVIDIA RTX A5000
                gpu_count: 2
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 8192
                      memoryTotal: "25757220864"
                      name: NVIDIA RTX A5000
                      uuid: GPU-1791a6c7-6fc1-bc6e-2044-468c38566b15
                    - architecture: Ampere
                      cudaCores: 8192
                      memoryTotal: "25757220864"
                      name: NVIDIA RTX A5000
                      uuid: GPU-ae573233-dfff-33b9-58c5-1f9ab99fcb35
                host: mirzakhani
                memory:
                    total: "201182593024"
                os: Linux-5.15.0-139-generic-x86_64-with-glibc2.31
                program: -m forward_sft
                python: CPython 3.11.13
                root: /homes/pr450/repos/mr_repos/forced_recog
                startedAt: "2025-08-31T20:17:36.148399Z"
                writerId: ldrbasgq9dmimqcbc6nl655staajz37n
        m: []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
                - 98
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
                - 98
            "3":
                - 13
                - 16
            "4": 3.11.13
            "5": 0.21.3
            "6": 4.53.1
            "12": 0.21.3
            "13": linux-x86_64
dataset:
    value: cnn_dailymail
model_name:
    value: meta-llama/Llama-3.1-8B-Instruct
run_name:
    value: llama_3_forward_training_test
sft_config:
    value:
        batch_size: 4
        learning_rate: "5e-4"
        lora_alpha: 32
        lora_dropout: 0.1
        lora_r: 16
        max_seq_length: 2048
        num_epochs: 3
        target_modules:
            - q_proj
            - v_proj
style:
    value: sun
temperature:
    value: 0
