model_name: "meta-llama/Llama-3.1-8B-Instruct"
dataset: cnn_dailymail

temps: [0.0, 0.0, 0.0]
num_trials: [1, 1, 1]
styles: [sun, economist, natural]

splits: [test, train] # Skipping others for now

wandb_project_name: selfrecognition_0925

forwardsft_temp0.0_stylesun:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  learning_rate: 0.0005
  num_epochs: 3
  batch_size: 1
  max_seq_length: 2048
