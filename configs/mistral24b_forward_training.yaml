model_name: "mistralai/Mistral-Small-24B-Instruct-2501"
dataset: cnn_dailymail

temps: [0.0, 0.0, 0.0]
num_trials: [1, 1, 1]
styles: [sun, economist, natural]

splits: [test] # Skipping others for now

wandb_project_name: selfrecognition_0925

forwardsft_temp0.0_stylesun:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  learning_rate: 0.0001
  num_epochs: 1
  max_steps: 20
  save_frequency: 1
  batch_size: 1
  max_seq_length: 2048

forwardsft_temp0.0_styleeconomist:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  learning_rate: 0.0001
  num_epochs: 1
  max_steps: 20
  save_frequency: 1
  batch_size: 1
  max_seq_length: 2048
